{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `BrainTask`s or how to customize your TPS simulation\n",
    "\n",
    "As already mentioned in the example notebooks that show how to run a TPS simulation with `aimmd.distributed`, the central object of the TPS simulation (the `Brain`) runs all its `BrainTask`s after every Monte Carlo step. This is very similar to the concept of a `hook` in `openpathsampling` with the difference that `openpathsampling` defines pre- and post-step hooks, while in `aimmd.distributed` the `BrainTask`s are only called after the Monte Carlo step, i.e. only post-step hooks are currently implemented.\n",
    "\n",
    "In addition the the predefined `BrainTask`s to e.g. train the model, save the trainset, and perform the density collection (for the density correction in $\\phi_B$-space), users can easily define their own `BrainTask`s to modify the behavior of their TPS simulation. To this end one just needs to subclass the `BrainTask` abstract base class and attach the resulting user-defined `BrainTask` to the `Brain` as usual.\n",
    "\n",
    "**Required knowledge/recommended reading**: This notebook assumes that you are familiar with setting up and running a TPS simulation using `aimmd.distributed`. If you are not familliar with running an `aimmd.distributed` TPS simulation, please have a look at the notebooks `TPS_1_setup_and_run_simulation.ipynb` to `TPS_4_rerun_with_changed_parameters_or_recover_crashed_simulations.ipynb` or `TPS_with_EQ_SPs_1_generate_SPs_from_UmbrellaSampling.ipynb` to `TPS_with_EQ_SPs_4_rerun_with_changed_parameters_or_recover_crashed_simulations.ipynb` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not initialize SLURM cluster handling. If you are sure SLURM (sinfo/sacct/etc) is available try calling `asyncmd.config.set_slurm_settings()` with the appropriate arguments.\n",
      "/Users/hejung/miniconda3/envs/aimmd_test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:aimmd:dCGPy not found. SymReg will not be available.\n",
      "WARNING:aimmd:Tensorflow/Keras not available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import asyncmd\n",
    "import asyncmd.gromacs as asyncgmx\n",
    "from asyncmd import Trajectory\n",
    "import aimmd\n",
    "import aimmd.distributed as aimmdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup working directory\n",
    "#scratch_dir = \"/homeloc/scratch/aimmd_distributed/\"\n",
    "scratch_dir = \".\"\n",
    "\n",
    "workdir = os.path.join(scratch_dir, \"TransitionPathSampling_ala_customizing_TPS_with_BrainTasks\")\n",
    "if not os.path.isdir(workdir):\n",
    "    os.mkdir(workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined `BrainTask`s\n",
    "\n",
    "As mentioned above, we will subclass the `aimmd.distributed.pathsampling.BrainTask` abstract base class to define our own `BrainTask`.\n",
    "\n",
    "The only two methods we need to define are `__init__` (to initialize our class) and `run` (which will be called after every Monte Carlo step by the `Brain`). The `run` method will be called with three arguments: the `Brain` which performs the simulation, the Monte Carlo step that just finished and the index of the sampler that produced the Monte carlo step.\n",
    "\n",
    "Below are two (somewhat useless) examples for user-defined `BrainTask`s:\n",
    "\n",
    "- The `VerbosePrintTask` just prints some info every time it gets called with a Monte Carlo step. Note that its job can be done by the `Brain` itself if you call the `Brain`s `run_for_n_steps` or `run_for_n_accepts` methods with `print_progress=1` when running the simulation.\n",
    "\n",
    "- The `StupidTask` does something arguably stupid, namely that it breaks the Markov Chain Monte Carlo by modifying the Monte Carlo step to a non-accepted step if its acceptance probability is smaller than `p_cut`. This is done here mostly to showcase that `BrainTask`s are a powerfull tool which enables you to do (almost) arbitrary things to modify the behavior of your TPS simulation, if they make sense is another story. As always: With great power comes great responsibility and great potential for mistakes ;)\n",
    "\n",
    "Note that each `BrainTask` will be run if the stepnumber is divisible by `interval`, e.g. a `BrainTask` with `interval=3` will only be run after the 3rd, 6th, 9th, etc. Monte Carlo step. Furthermore, `BrainTasks` are called in the order in which they are passed to the `Brain`, e.g. if you pass `tasks= [VerbosePrintTask(), StupidTask()]` then the `VerbosePrintTask` will always run before the `StupidTask` (at least if they both are supposed to run at a given stepnumber)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aimmd.distributed\n",
    "from aimmd.distributed.pathmovers import MCstep  # only needed for the type hints\n",
    "\n",
    "\n",
    "class VerbosePrintTask(aimmd.distributed.pathsampling.BrainTask):\n",
    "    def __init__(self, interval: int = 1, name: str = \"VerbosePrintTask\"):\n",
    "        super().__init__(interval)\n",
    "        self._call_count = 0\n",
    "        self.name = name\n",
    "\n",
    "    def run(self, brain, mcstep: MCstep, sampler_idx: int):\n",
    "        # This will be called every `interval` finished Monte Carlo steps\n",
    "        self._call_count += 1  # increment call count to see how many times we call run\n",
    "        # We just print some basic info\n",
    "        print(f\"A sampler ({brain.samplers[sampler_idx]}) (index={sampler_idx}) \"\n",
    "              f\"attached to {brain} produced a Monte Carlo step ({mcstep}). \"\n",
    "              f\"This BrainTask with name {self.name} ({self}) got called for the {self._call_count}th time.\"\n",
    "              )\n",
    "\n",
    "\n",
    "class StupidTask(aimmd.distributed.pathsampling.BrainTask):\n",
    "    def __init__(self, interval: int = 1, p_cut: float = 0.5):\n",
    "        super().__init__(interval)\n",
    "        self.p_cut = p_cut\n",
    "\n",
    "    def run(self, brain, mcstep: MCstep, sampler_idx: int):\n",
    "        if mcstep.p_acc <= self.p_cut:\n",
    "            mcstep.accepted = False\n",
    "\n",
    "\n",
    "class MakeEveryStepAcceptedTask(aimmd.distributed.pathsampling.BrainTask):\n",
    "    def __init__(self, interval: int = 1):\n",
    "        super().__init__(interval)\n",
    "\n",
    "    def run(self, brain, mcstep: MCstep, sampler_idx: int):\n",
    "        if not mcstep.accepted:\n",
    "            # modify non-accepted steps to be accepted\n",
    "            # NOTE: this will (most likely) crash a sequential TPS simulation\n",
    "            #       because steps that are not accepted do not need to contain\n",
    "            #       a valid transition (i.e. their `path` attribute is not set)\n",
    "            #       so we can not start a new Monte Carlo step by shooting from\n",
    "            #       the last transition path\n",
    "            #       In a TPS simulation with equilibrium shooting points this\n",
    "            #       BrainTask will not have any effect becasue there every step\n",
    "            #       is formally accepted (and has an associated weight which\n",
    "            #       can be zero)\n",
    "            mcstep.accepted = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the TPS simulation\n",
    "\n",
    "This is the same setup as used in `TPS_1_setup_and_run_simulation.ipynb`, just with less comments and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResUnit 1 is 66 units wide.\n",
      "Dropout before it is 0.18674307214231128.\n",
      "ResUnit 2 is 41 units wide.\n",
      "Dropout before it is 0.1162432499771616.\n",
      "ResUnit 3 is 25 units wide.\n",
      "Dropout before it is 0.07235873872180604.\n",
      "ResUnit 4 is 16 units wide.\n",
      "Dropout before it is 0.045041643884176266.\n",
      "ResUnit 5 is 10 units wide.\n",
      "Dropout before it is 0.02803738317757008.\n"
     ]
    }
   ],
   "source": [
    "# number of samplers\n",
    "n_samplers = 2  # results in 2*n_samplers gmx engines\n",
    "\n",
    "# storage file\n",
    "storage = aimmd.Storage(os.path.join(workdir, \"storage.h5\"))\n",
    "\n",
    "# state functions and descriptor transform function\n",
    "os.chdir(\"..\")  # chdir to one level above to import state_funcs_mda.py\n",
    "from state_funcs_mda import alpha_R, C7_eq, descriptor_func_ic\n",
    "os.chdir(\"Advanced_topics\")  # and back to the folder in which we run the notebook\n",
    "# state functions\n",
    "wrapped_alphaR = asyncmd.trajectory.PyTrajectoryFunctionWrapper(alpha_R)\n",
    "wrapped_C7_eq = asyncmd.trajectory.PyTrajectoryFunctionWrapper(C7_eq)\n",
    "# descriptor transform\n",
    "# descriptor_func_ic gives us an internal coordinate representation (i.e. bond lengths, angles and dihedrals)\n",
    "wrapped_transform = asyncmd.trajectory.PyTrajectoryFunctionWrapper(descriptor_func_ic,\n",
    "                                                                   call_kwargs={\"molecule_selection\": \"protein\"},\n",
    "                                                                   )\n",
    "\n",
    "# Underlying dynamics/ Define the engine(s) for the PathMovers (they will all be the same)\n",
    "gro = \"../gmx_infiles/conf.gro\"\n",
    "top = \"../gmx_infiles/topol_amber99sbildn.top\"\n",
    "ndx = \"../gmx_infiles/index.ndx\"\n",
    "mdp = asyncgmx.MDP(\"../gmx_infiles/md.mdp\")\n",
    "gmx_engine_kwargs = {\"mdconfig\": mdp,\n",
    "                     \"gro_file\": gro,\n",
    "                     \"top_file\": top,\n",
    "                     \"ndx_file\": ndx,\n",
    "                     \"output_traj_type\": \"XTC\",\n",
    "                     #\"mdrun_extra_args\": \"-nt 2\",\n",
    "                     # use this for gmx sans (thread) MPI\n",
    "                     \"mdrun_extra_args\": \"-ntomp 2\",\n",
    "                     }\n",
    "gmx_engine_cls = asyncgmx.GmxEngine\n",
    "\n",
    "# initial transition\n",
    "tp_initial = Trajectory(structure_file=\"../gmx_infiles/ala_300K_amber99sb-ildn.tpr\",\n",
    "                        trajectory_files=\"../gmx_infiles/TP_low_barrier_300K_amber99sbildn.trr\",\n",
    "                        )\n",
    "\n",
    "# Model definition\n",
    "# first get the descriptors for them to infer the number of inputs for our model\n",
    "descriptors_for_initial_tp = await wrapped_transform(tp_initial)\n",
    "# architecture specification\n",
    "n_lay_pyramid = 5  # number of resunits\n",
    "n_unit_top = 10  # number of units in the last layer before the log_predictor\n",
    "dropout_base = 0.3  # dropot fraction in the first layer (will be reduced going to the top)\n",
    "n_unit_base = cv_ndim = descriptors_for_initial_tp.shape[1]  # input dimension\n",
    "# the factor by which we reduce the number of units per layer (the width) and the dropout fraction\n",
    "fact = (n_unit_top / n_unit_base)**(1./(n_lay_pyramid))\n",
    "\n",
    "modules = []\n",
    "for i in range(1, n_lay_pyramid + 1):\n",
    "    modules += [aimmd.pytorch.networks.FFNet(n_in=max(n_unit_top, int(n_unit_base * fact**(i-1))),\n",
    "                                             n_hidden=[max(n_unit_top, int(n_unit_base * fact**i))],  # 1 hidden layer network\n",
    "                                             activation=torch.nn.Identity(),\n",
    "                                             dropout={\"0\": dropout_base * fact**i}\n",
    "                                             )\n",
    "                ]\n",
    "    print(f\"ResUnit {i} is {max(n_unit_top, int(n_unit_base * fact**(i)))} units wide.\")\n",
    "    print(f\"Dropout before it is {dropout_base * fact**i}.\")\n",
    "    modules += [aimmd.pytorch.networks.ResNet(n_units=max(n_unit_top, int(n_unit_base * fact**i)),\n",
    "                                              n_blocks=1)\n",
    "                ]\n",
    "torch_model = aimmd.pytorch.networks.ModuleStack(n_out=1, modules=modules)\n",
    "# move model to GPU if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    torch_model = torch_model.to('cuda')\n",
    "# optimizer to train the model\n",
    "optimizer = torch.optim.Adam(torch_model.parameters(), lr=1e-3)\n",
    "# wrapp the pytorch neural network model in a RCModel class,\n",
    "model = aimmd.pytorch.EEScalePytorchRCModelAsync(nnet=torch_model,\n",
    "                                                 optimizer=optimizer,\n",
    "                                                 states=[wrapped_C7_eq, wrapped_alphaR],\n",
    "                                                 ee_params={'lr_0': 1e-3,\n",
    "                                                            'lr_min': 5e-5,\n",
    "                                                            'epochs_per_train': 3,\n",
    "                                                            'window': 100,\n",
    "                                                            'batch_size': 8192,\n",
    "                                                           },\n",
    "                                                 descriptor_transform=wrapped_transform,\n",
    "                                                 cache_file=storage,\n",
    "                                                 )\n",
    "\n",
    "# Define the TPS sampling scheme\n",
    "# shooting point selection\n",
    "spselector = aimmdd.spselectors.RCModelSPSelectorFromTraj()\n",
    "# and setup the movers lists (i.e. mover_cls and mover_kwargs for each sampler)\n",
    "# here we just use one move-type per sampler and therefore only have one entry\n",
    "# in each list\n",
    "movers_cls = [aimmdd.pathmovers.TwoWayShootingPathMover]\n",
    "movers_kwargs = [{'states': [wrapped_alphaR, wrapped_C7_eq],\n",
    "                  'engine_cls': gmx_engine_cls,\n",
    "                  'engine_kwargs': gmx_engine_kwargs,\n",
    "                  'walltime_per_part': 0.00003125,  # 0.1125 s per part\n",
    "                  #'walltime_per_part': 0.0000625,  # 0.225 s per part\n",
    "                  'T': mdp[\"ref-t\"][0],\n",
    "                  \"sp_selector\": spselector,\n",
    "                  \"max_steps\": 500 * 10**5,  # 500 steps * dt (2 fs) = 1 ps\n",
    "                  }\n",
    "                 ]\n",
    "\n",
    "# Trainset\n",
    "trainset = aimmd.TrainSet(n_states=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize `BrainTask`s\n",
    "\n",
    "Here we will initialize our user-defined `BrainTask`s and also the basic `BrainTask`s needed for a TPS simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    # TrainingTask\n",
    "    aimmdd.pathsampling.TrainingTask(model=model, trainset=trainset),\n",
    "    # SaveTask\n",
    "    aimmdd.pathsampling.SaveTask(storage=storage, model=model, trainset=trainset),\n",
    "    # DensityCollectionTask\n",
    "    aimmdd.pathsampling.DensityCollectionTask(model=model,\n",
    "                                              first_collection=100,\n",
    "                                              recreate_interval=250,\n",
    "                                              ),\n",
    "    # this task will print after every finished Monte Carlo step\n",
    "    VerbosePrintTask(interval=1, name=\"VerbosePrintTask_interval=1\"),\n",
    "    # this task will print only every 3rd finished Monte Carlo step\n",
    "    VerbosePrintTask(interval=3, name=\"VerbosePrintTask_interval=3\"),\n",
    "    # p_cut=100 should result in no accepted Monte Carlo steps as we will only\n",
    "    # accept steps that have p_acc >= 100\n",
    "    # (p_cut=0. would result in no modified Monte Carlo steps at all)\n",
    "    StupidTask(interval=1, p_cut=100.),\n",
    "    # uncomment the next line if you want to crash your TPS simulation :)\n",
    "    #MakeEveryStepAcceptedTask(interval=1),\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `Brain` (and attach the `BrainTask`s)\n",
    "\n",
    "Also seed the initial transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = aimmdd.Brain.samplers_from_moverlist(model=model, workdir=workdir, storage=storage,\n",
    "                                             n_sampler=n_samplers,\n",
    "                                             movers_cls=movers_cls, movers_kwargs=movers_kwargs,\n",
    "                                             samplers_use_same_stepcollection=False,\n",
    "                                             tasks=tasks)\n",
    "# seed initial transition for each Markov chain sampler\n",
    "brain.seed_initial_paths(trajectories=[tp_initial])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "gmx must be an existing path or accesible via the $PATH environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# we should call the second VerbosePrintTask 2 times and the first one 7 times\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m brain\u001b[38;5;241m.\u001b[39mrun_for_n_steps(\u001b[38;5;241m7\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/aimmd/aimmd/distributed/pathsampling.py:1036\u001b[0m, in \u001b[0;36mBrain.run_for_n_steps\u001b[0;34m(self, n_steps, print_progress)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;66;03m# iterate over all done results, because there can be multiple\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;66;03m# done sometimes?\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m     sampler_idx \u001b[38;5;241m=\u001b[39m sampler_tasks\u001b[38;5;241m.\u001b[39mindex(result)\n\u001b[0;32m-> 1036\u001b[0m     mcstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m result\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mcstep \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         \u001b[38;5;66;03m# bail out because this mcstep was a dummy sampler, i.e. None\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/aimmd/aimmd/distributed/pathsampling.py:1337\u001b[0m, in \u001b[0;36mPathChainSampler.run_step\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1335\u001b[0m mover \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovers, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmover_weights)\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;66;03m# and run the step\u001b[39;00m\n\u001b[0;32m-> 1337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_step(model\u001b[38;5;241m=\u001b[39mmodel, instep\u001b[38;5;241m=\u001b[39minstep, mover\u001b[38;5;241m=\u001b[39mmover,\n\u001b[1;32m   1338\u001b[0m                             continuation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/aimmd/aimmd/distributed/pathsampling.py:1368\u001b[0m, in \u001b[0;36mPathChainSampler._run_step\u001b[0;34m(self, model, mover, instep, continuation)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;66;03m# and do the actual step\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     outstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m mover\u001b[38;5;241m.\u001b[39mmove(instep\u001b[38;5;241m=\u001b[39minstep, stepnum\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stepnum,\n\u001b[1;32m   1369\u001b[0m                                wdir\u001b[38;5;241m=\u001b[39mstep_dir, model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1370\u001b[0m                                continuation\u001b[38;5;241m=\u001b[39mcontinuation,\n\u001b[1;32m   1371\u001b[0m                                )\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxStepsReachedError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;66;03m# error raised when any trial takes \"too long\" to commit\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampler \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: MaxStepsReachedError, retrying MC \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1375\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler_idx,\n\u001b[1;32m   1376\u001b[0m                  )\n",
      "File \u001b[0;32m~/Documents/aimmd/aimmd/distributed/pathmovers.py:230\u001b[0m, in \u001b[0;36mModelDependentPathMover.move\u001b[0;34m(self, instep, stepnum, wdir, model, continuation, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove\u001b[39m(\u001b[38;5;28mself\u001b[39m, instep: MCstep, stepnum: \u001b[38;5;28mint\u001b[39m, wdir: \u001b[38;5;28mstr\u001b[39m, model,\n\u001b[1;32m    225\u001b[0m                continuation: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MCstep:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# this enables us to reuse the save/delete logic for every\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# modeldependant pathmover\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_move(instep\u001b[38;5;241m=\u001b[39minstep, stepnum\u001b[38;5;241m=\u001b[39mstepnum, wdir\u001b[38;5;241m=\u001b[39mwdir,\n\u001b[1;32m    229\u001b[0m                          model\u001b[38;5;241m=\u001b[39mmodel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 230\u001b[0m     outstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move(instep\u001b[38;5;241m=\u001b[39minstep, stepnum\u001b[38;5;241m=\u001b[39mstepnum, wdir\u001b[38;5;241m=\u001b[39mwdir,\n\u001b[1;32m    231\u001b[0m                                continuation\u001b[38;5;241m=\u001b[39mcontinuation, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_move(instep\u001b[38;5;241m=\u001b[39minstep, stepnum\u001b[38;5;241m=\u001b[39mstepnum, wdir\u001b[38;5;241m=\u001b[39mwdir,\n\u001b[1;32m    233\u001b[0m                           \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outstep\n",
      "File \u001b[0;32m~/Documents/aimmd/aimmd/distributed/pathmovers.py:493\u001b[0m, in \u001b[0;36m_TwoWayShootingPathMoverMixin._move\u001b[0;34m(self, instep, stepnum, wdir, continuation, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m             tt\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;66;03m# raise the exception, we take care of retrying complete steps\u001b[39;00m\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;66;03m# in the PathSamplingChain\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m t\u001b[38;5;241m.\u001b[39mexception() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# if no task raised an exception all should done, so get the results\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pending) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# but make sure everything went as we expect\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/asyncmd/src/asyncmd/trajectory/propagate.py:741\u001b[0m, in \u001b[0;36mConditionalTrajectoryPropagator.propagate\u001b[0;34m(self, starting_configuration, workdir, deffnm, continuation)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trajs, first_condition_fullfilled\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# starting configuration does not fullfill any condition, lets do MD\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continuation:\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# continuation: get all traj parts already done and continue from\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# there, i.e. append to the last traj part found\u001b[39;00m\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;66;03m# NOTE: we assume that the condition functions could be different\u001b[39;00m\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;66;03m# so get all traj parts and calculate the condition funcs on them\u001b[39;00m\n\u001b[1;32m    747\u001b[0m     trajs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_all_traj_parts(folder\u001b[38;5;241m=\u001b[39mworkdir, deffnm\u001b[38;5;241m=\u001b[39mdeffnm,\n\u001b[1;32m    748\u001b[0m                                      engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m    749\u001b[0m                                      )\n",
      "File \u001b[0;32m~/Documents/asyncmd/src/asyncmd/gromacs/mdengine.py:187\u001b[0m, in \u001b[0;36mGmxEngine.__init__\u001b[0;34m(self, mdconfig, gro_file, top_file, ndx_file, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndx_file \u001b[38;5;241m=\u001b[39m ndx_file  \u001b[38;5;66;03m# sets self._ndx_file\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# dirty hack to make sure we also check for our defaults if they are\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# available + executable\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmdrun_executable\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmdrun_executable\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrompp_executable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrompp_executable\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# same for output traj type, check if it is in allowed values and\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# TODO: in the future we want to possibly check if that traj type is\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# actually written with current mdp settings?\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/asyncmd/src/asyncmd/gromacs/mdengine.py:82\u001b[0m, in \u001b[0;36m_descriptor_check_executable.__set__\u001b[0;34m(self, obj, val)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__set__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, val):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# split because mdrun and grompp can be both subcommands of gmx\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     test_exe \u001b[38;5;241m=\u001b[39m val\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mensure_executable_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_exe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__set__\u001b[39m(obj, val)\n",
      "File \u001b[0;32m~/Documents/asyncmd/src/asyncmd/tools.py:52\u001b[0m, in \u001b[0;36mensure_executable_available\u001b[0;34m(executable)\u001b[0m\n\u001b[1;32m     50\u001b[0m     executable \u001b[38;5;241m=\u001b[39m shutil\u001b[38;5;241m.\u001b[39mwhich(executable)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecutable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be an existing path or accesible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m                      \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvia the $PATH environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m executable\n",
      "\u001b[0;31mValueError\u001b[0m: gmx must be an existing path or accesible via the $PATH environment variable."
     ]
    }
   ],
   "source": [
    "# we should call the second VerbosePrintTask 2 times and the first one 7 times\n",
    "await brain.run_for_n_steps(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on mcstepcollection with index 0.\n",
      "    Step with index 0: MCStep(mover=None, stepnum=0, states_reached=None, accepted=True, p_acc=1, predicted_committors_sp=None, weight=1,\n",
      "       directory=TransitionPathSampling_ala_customizing_TPS_with_BrainTasks/sampler_0/mcstep_0)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Working on mcstepcollection with index 1.\n",
      "    Step with index 0: MCStep(mover=None, stepnum=0, states_reached=None, accepted=True, p_acc=1, predicted_committors_sp=None, weight=1,\n",
      "       directory=TransitionPathSampling_ala_customizing_TPS_with_BrainTasks/sampler_1/mcstep_0)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "All steps have been rejected: True\n"
     ]
    }
   ],
   "source": [
    "# check that no MCStep is accepted (if its acceptance probability is smaller than 100)\n",
    "all_steps = []\n",
    "for c_idx, collection in enumerate(storage.mcstep_collections):\n",
    "    print(f\"Working on mcstepcollection with index {c_idx}.\")\n",
    "    for s_idx, step in enumerate(collection):\n",
    "        print(f\"    Step with index {s_idx}: {step}\")\n",
    "        all_steps += [step]\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "print(f\"All steps have been rejected: {all(s.accepted for s in all_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the storage\n",
    "storage.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimmd_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
