{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tensorflow/Keras not available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mdtraj as mdt\n",
    "import MDAnalysis as mda\n",
    "import aimmd\n",
    "import aimmd.distributed as aimmdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup working directory\n",
    "\n",
    "#scratch_dir = \".\"\n",
    "#scratch_dir = \"/home/tb/hejung/DATA/aimmd_scratch/aimmd_distributed/\"\n",
    "scratch_dir = \"/home/think/scratch/aimmd_distributed/\"\n",
    "\n",
    "workdir = os.path.join(scratch_dir, \"PathSampling_mda_test\")\n",
    "\n",
    "if not os.path.isdir(workdir):\n",
    "    os.mkdir(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "# executing this file sets the variable LOGCONFIG, which is a dictionary of logging presets \n",
    "%run ../resources/logconf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'level': 'WARN', 'handlers': ['stdf', 'warnout']}\n",
      "{'level': 'INFO'}\n",
      "{'class': 'logging.FileHandler', 'level': 'INFO', 'mode': 'w', 'filename': 'simulation.log', 'formatter': 'standardFormatter'}\n"
     ]
    }
   ],
   "source": [
    "# have a look at the default logging level (the level used for the root logger)\n",
    "print(LOGCONFIG[\"loggers\"][\"\"])\n",
    "# have a look at the logger for aimmd\n",
    "print(LOGCONFIG[\"loggers\"][\"aimmd\"])\n",
    "# and have a look at the log-level for the filehandler\n",
    "print(LOGCONFIG[\"handlers\"][\"stdf\"])\n",
    "# the last two should both be `INFO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: more logging\n",
    "#LOGCONFIG[\"handlers\"][\"stdf\"][\"level\"] = \"DEBUG\"\n",
    "#LOGCONFIG[\"loggers\"][\"aimmd\"][\"level\"] = \"DEBUG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can either modify single values or use it as is to get the same setup as in the OPS default logging config file\n",
    "# you could e.g. do LOGCONF['handlers']['stdf']['filename'] = new_name to change the filename of the log\n",
    "# the default is to create 'simulation.log' and 'initialization.log' in the current working directory\n",
    "import logging.config\n",
    "LOGCONFIG[\"handlers\"][\"stdf\"][\"filename\"] = os.path.join(workdir, \"simulation_pathsampling.log\")\n",
    "LOGCONFIG[\"handlers\"][\"initf\"][\"filename\"] = os.path.join(workdir, \"initlog_pathsampling.log\")\n",
    "logging.config.dictConfig(LOGCONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now the actual \"setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = aimmd.Storage(os.path.join(workdir, \"storage.h5\"), mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = storage.rcmodels[\"model_to_continue_with\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = storage.load_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [aimmdd.pathsampling.TrainingTask(model=model, trainset=trainset),\n",
    "         aimmdd.pathsampling.SaveTask(storage=storage, model=model, trainset=trainset),\n",
    "         aimmdd.pathsampling.DensityCollectionTask(model=model,\n",
    "                                                   first_collection=100,\n",
    "                                                   recreate_interval=500,\n",
    "                                                   interval=10\n",
    "                                                   ),\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = storage.central_memory.load_brain(model=model, tasks=tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(ERROR)aimmd.distributed.logic: Starting configuration (Trajectory(trajectory_file=/home/think/scratch/aimmd_distributed/PathSampling_mda_test/chain_2/mcstep_4/forward_SP.trr, structure_file=/home/think/Documents/sources/OPS/aimmd/examples/distributed/gmx_infiles/ala_300K_amber99sb-ildn.tpr)) is already inside the state with idx 0.\n",
      "(ERROR)aimmd.distributed.logic: Starting configuration (Trajectory(trajectory_file=/home/think/scratch/aimmd_distributed/PathSampling_mda_test/chain_2/mcstep_4/backward_SP.trr, structure_file=/home/think/Documents/sources/OPS/aimmd/examples/distributed/gmx_infiles/ala_300K_amber99sb-ildn.tpr)) is already inside the state with idx 0.\n"
     ]
    }
   ],
   "source": [
    "await brain.run_for_n_steps(500)\n",
    "#await brain.run_for_n_steps(2000)\n",
    "#await brain.run_for_n_accepts(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.log_train_decision[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the value of the learning rate over the course of training\n",
    "# note however, that we did not train at every step, but just at every interval MCsteps\n",
    "log_train = np.array(model.log_train_decision)\n",
    "lr = log_train[:,1]\n",
    "plt.plot(lr, label='lr')\n",
    "# see where we really trained: everywhere where train=True\n",
    "# set lr_true to NaN anywhere where we did not train to have a nice plot\n",
    "lr_true = lr\n",
    "lr_true[log_train[:,0] == False] = np.nan\n",
    "plt.plot(lr_true, '+', label='True learning')\n",
    "# lr_min as a guide to the eye\n",
    "plt.axhline(model.ee_params['lr_min'], label='lr_min', color='lime')\n",
    "plt.legend()\n",
    "plt.xlabel('MCStep', size=15);\n",
    "plt.ylabel('Learning rate', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model losses at each step where it trained\n",
    "# this will be epochs_per_training loss values per training\n",
    "plt.plot(model.log_train_loss, label='training loss')\n",
    "plt.legend();\n",
    "plt.ylabel('loss per training point', size=15)\n",
    "plt.xlabel('training step', size=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resort such that we have a loss value per MCStep, NaN if we did not train at that step\n",
    "train_loss = []\n",
    "count = 0\n",
    "for t in log_train[:, 0]:\n",
    "    if t:\n",
    "        train_loss.append(model.log_train_loss[count])\n",
    "        count += 1\n",
    "    else:\n",
    "        train_loss.append([np.nan for _ in range(model.ee_params['epochs_per_train'])])\n",
    "    \n",
    "plt.plot(train_loss, '+', label='training loss')\n",
    "plt.legend();\n",
    "plt.ylabel('loss per training point', size=15)\n",
    "plt.xlabel('MCStep', size=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find a way, currently it is not possibly to get the accepts in the same order as the trainset?!\n",
    "# TODO: probably we will have to write a DataCollectionTask to achieve that ?!\n",
    "# get accepts from storage:\n",
    "# NOTE: this is just roughly the right order\n",
    "accepts = []\n",
    "for accs in zip(*(c.accepts for c in brain.chains)):\n",
    "    accepts += list(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot efficiency, expected efficiency and accepts\n",
    "# Note: this will only work for models with n_out=1, due to the way we calculate p(TP|x)\n",
    "\n",
    "p_ex = np.array(model.expected_p)\n",
    "\n",
    "l, = plt.plot(np.cumsum(trainset.transitions), label='generated');\n",
    "plt.plot(np.cumsum(accepts), c=l.get_color(), ls='--', label='accepted');\n",
    "plt.plot(np.cumsum(2*p_ex*(1 - p_ex)),c=l.get_color(), ls=':', label='expected');\n",
    "plt.plot(np.cumsum(2*p_ex*(1 - p_ex))- np.cumsum(trainset.transitions), label='diff (expected - generated)')\n",
    "plt.plot(np.linspace(0., len(trainset)/2., len(trainset)), c='k', ls='--', label='maximal', lw=2)\n",
    "plt.legend(fontsize=12);\n",
    "plt.ylabel('Cummulative count of TPs', size=15)\n",
    "plt.xlabel('cummulative MC Steps', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hipr = aimmd.analysis.HIPRanalysis(model, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hipr_plus_losses, hipr_plus_stds = hipr.do_hipr_plus(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_diffs = hipr_plus_losses[:-1] - hipr_plus_losses[-1]  # hipr_losses[-1] is the reference loss over the unaltered trainset\n",
    "\n",
    "plt.bar(np.arange(len(loss_diffs)), loss_diffs, yerr=hipr_plus_stds[:-1])\n",
    "plt.xlabel('Coordinate index', size=15)\n",
    "plt.ylabel('Relative importance', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from state_funcs_mda import generate_atomgroups_for_ic\n",
    "\n",
    "u = mda.Universe(\"gmx_infiles/ala_300K_amber99sb-ildn.tpr\", \"gmx_infiles/conf.gro\",\n",
    "                 refresh_offsets=True, tpr_resid_from_one=True)\n",
    "molecule = u.select_atoms('protein')\n",
    "pairs, triples, quadruples = generate_atomgroups_for_ic(molecule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the most important contributors?\n",
    "max_idxs = np.argsort(loss_diffs)[::-1]\n",
    "sf_parms = {}\n",
    "\n",
    "#pairs, triples, quadruples = aimmd.coords.internal.generate_indices(traj.topology, source_idx=1)\n",
    "\n",
    "ic_parms = {\"pairs\": pairs, \"triples\": triples, \"quadruples\": quadruples}\n",
    "\n",
    "print('reference loss:', hipr_plus_losses[-1])\n",
    "for idx in max_idxs[:10]:\n",
    "    print()\n",
    "    print('loss for idx {:d}: '.format(idx), hipr_plus_losses[idx])\n",
    "    if idx < len(pairs[0]):\n",
    "        print(f\"bond between: {pairs[0][idx]} and {pairs[1][idx]}\")\n",
    "        continue\n",
    "    idx -= len(pairs[0])\n",
    "    if idx < len(triples[0]):\n",
    "        print(f\"angle between {triples[0][idx]}, {triples[1][idx]} and {triples[2][idx]}\")\n",
    "        continue\n",
    "    idx -= len(triples[0])\n",
    "    if idx % 2 == 0:\n",
    "        st = \"sinus\"\n",
    "    else:\n",
    "        st = \"cosinus\"\n",
    "    st += f\" of dihedral between {quadruples[0][idx // 2]}, {quadruples[1][idx // 2]}, {quadruples[2][idx // 2]} and {quadruples[3][idx // 2]}.\"\n",
    "    print(st)\n",
    "    #print(aimmd.coords.get_involved(idx, sf_parms=sf_parms, ic_parms=ic_parms, solvent_atoms=[['O', 'H']], solvent_resname=['HOH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the last model\n",
    "storage.rcmodels[\"model_to_continue_with\"] = model\n",
    "storage.save_trainset(trainset)\n",
    "# the rest should be saved automatically\n",
    "# maybe we should save the model + trainset at the end of the simulation autmatically too?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIMMD nature publish (py3.7.3/June-2021)",
   "language": "python",
   "name": "aimmd_nature_publish"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
